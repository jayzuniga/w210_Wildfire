{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudstorage as gcs\n",
    "import glob\n",
    "import gc\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas_gbq\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import geopandas as gpd\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from joblib import dump,load\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load each csv into a pandas DF\n",
    "\n",
    "def load_data(path=None):\n",
    "    \"\"\"Loads the csvs into pandas df\n",
    "    \n",
    "    Kwargs:\n",
    "        path - str - path to csv data files\n",
    "    Returns:\n",
    "        wildfire_data - Pandas DF - DF of all data\"\"\"\n",
    "    \n",
    "    if not path:\n",
    "        path = r'../../full_dataset' # use your path\n",
    "        \n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    li = []\n",
    "    i = 0\n",
    "    for filename in all_files:\n",
    "        print(filename)\n",
    "        if i == 0:\n",
    "            wildfire_data = pd.read_csv(filename)\n",
    "        else:\n",
    "            wildfire_data = wildfire_data.append(pd.read_csv(filename))\n",
    "        i += 1\n",
    "        print(wildfire_data.shape)\n",
    "        break\n",
    "    \n",
    "    wildfire_data.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return wildfire_data\n",
    "\n",
    "def preprocess_dataset(wildfire_data,downsample_size=None):\n",
    "    \"\"\"Fills in NA's, creates train/test split, removes unused cols\n",
    "    Args :\n",
    "        wildfire_data - Pandas DF - consolidated dataset\n",
    "    Returns :\n",
    "        train - Pandas DF - 2016-2017 input data\n",
    "        test - Pandas DF - 2018 input data\n",
    "        y_train - numpy array - 2016-2017 labels\n",
    "        y_test - numpy array - 2018 labels \n",
    "        indexer - Pandas DF - lookup for results Data/S2Cell by index\n",
    "        downsample_size - float - percentage of negative samples to include in Train\n",
    "    \"\"\"\n",
    "    \n",
    "    fuel_mean = 78.74 # Calculated in BigQuery\n",
    "    wildfire_data.fuel_percent.replace('backfill',str(fuel_mean),inplace = True)\n",
    "    wildfire_data['fuel_percent'] = pd.to_numeric(wildfire_data.fuel_percent)\n",
    "\n",
    "    # DF used to resolve which date/s2 cell each prediction corresponds too\n",
    "    indexer = wildfire_data[['s2_cell_id','measure_date']].copy()\n",
    "\n",
    "    train = wildfire_data[wildfire_data.measure_date < '2018-01-01'].copy()\n",
    "    test = wildfire_data[wildfire_data.measure_date >= '2018-01-01'].copy()\n",
    "\n",
    "    if downsample_size:\n",
    "        train = train[train.wf_wildfire.fillna(0) == 0].sample(frac=downsample_size).append(train[train.wf_wildfire > 0])\n",
    "        gc.collect()\n",
    "    y_train = train.wf_wildfire.fillna(0).copy().values\n",
    "    y_test = test.wf_wildfire.fillna(0).copy().values\n",
    "    \n",
    "    y_train[y_train > 0] = 1\n",
    "    y_test[y_test > 0] = 1\n",
    "    \n",
    "    # TODO identify any additional/engineered features to include\n",
    "    feature_cols = ['tl_object_id','fuel_percent',\n",
    "                    'wea_air_temp_max', 'wea_air_temp_mean', \n",
    "                    'wea_precip_accum_max', 'relative_humidity_max',\n",
    "                    'relative_humidity_min', 'relative_humidity_mean',\n",
    "                    'wea_wind_speed_max', 'wea_wind_speed_min', 'wea_wind_speed_mean',\n",
    "                    'wind_gust_max',\n",
    "                    'sat_faparval_min',\n",
    "                    'sat_faparval_max', 'sat_faparval_mean', \n",
    "                    'sat_faparval_median'\n",
    "                   ]\n",
    "\n",
    "    train = train[feature_cols]\n",
    "    test = test[feature_cols]\n",
    "\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "    train['fuel_percent'] = pd.to_numeric(train.fuel_percent)\n",
    "    test['fuel_percent'] = pd.to_numeric(test.fuel_percent)\n",
    "\n",
    "    del wildfire_data\n",
    "    gc.collect()\n",
    "\n",
    "    return train,test,y_train,y_test,indexer\n",
    "\n",
    "def train_models(models,train_data,train_labels,scaler=None,save=True):\n",
    "    \"\"\"Given a list of Sklearn models returns a list of trained models\n",
    "    \n",
    "    Args:\n",
    "        model - list of Sklearn model objects - models to be trained\n",
    "        train_data - Pandas DF - preprocessed training data\n",
    "        train_labels - Numpy Array - training data labels\n",
    "        scaler - Sklearn Scaler Object - [Optional]  \n",
    "        save - Bool - if true writes the models to disk \n",
    "    \"\"\"\n",
    "    trained_models = []\n",
    "    model_path = 'wildfire_{}.joblib'\n",
    "    if save == False:\n",
    "        trained_models = [load(model) for model in glob.glob('*.joblib')]\n",
    "    else:\n",
    "        if scaler:\n",
    "            for model in models:\n",
    "                model_name = model.__class__.__name__\n",
    "                print('Training -',model_name)\n",
    "                t0 = time.time()\n",
    "\n",
    "                trained_models.append(\n",
    "                    model.fit(scaler.fit_transform(train_data),train_labels))\n",
    "\n",
    "                dump(trained_models[-1], (model_path.format(model_name)))\n",
    "                print('Training time -',str(round(time.time() - t0,2))+'s')\n",
    "\n",
    "        else:\n",
    "            for model in models:\n",
    "                trained_models.append(\n",
    "                    model.fit(train_data,train_labels))\n",
    "\n",
    "                dump(trained_models[-1], (model_path.format(model_name)))\n",
    "    return trained_models\n",
    "    \n",
    "\n",
    "def make_probability_predictions(model,test_data,scaler=None):\n",
    "    \"\"\"Makes probability predictions on the validation data for each model\"\"\"\n",
    "    \n",
    "    if scaler:\n",
    "        if model.__class__.__name__ == 'IsolationForest':\n",
    "            predictions = ifc.decision_function(scaler.transform(test_data))\n",
    "        else:\n",
    "            predictions = model.predict_proba(scaler.transform(test_data))\n",
    "    else:\n",
    "        if model.__class__.__name__ == 'IsolationForest':\n",
    "            predictions = ifc.decision_function(test_data)\n",
    "        else:\n",
    "            predictions = model.predict_proba(test_data)\n",
    "    return predictions\n",
    "\n",
    "def binarize_predictions(predictions,threshold=.5):\n",
    "    \"\"\"Implements a custom classification threshold\"\"\"\n",
    "    \n",
    "    pred = np.zeros(len(predictions))\n",
    "    try:\n",
    "        if predictions.shape[1] == 2:\n",
    "            pred[[predictions[:,1] > threshold]] = 1\n",
    "    except: \n",
    "        pred[predictions > threshold] = 1\n",
    "    return pred\n",
    "\n",
    "def model_accuracies(y_test,predictions, model=None):\n",
    "    \"\"\"Calculates a variety of accuracy metrics\n",
    "    \n",
    "    Args:\n",
    "        y_test - Numpy Array - true labels\n",
    "        predictions - Numpy Array - binarized (0,1) model outputs\n",
    "        model - Sklearn Classifier \n",
    "    \n",
    "    \"\"\"\n",
    "    # # Accuracy Metrics\n",
    "    f1 = f1_score(y_test,predictions)\n",
    "    precision = precision_score(y_test,predictions)\n",
    "    recall = recall_score(y_test,predictions)\n",
    "\n",
    "    FP = np.sum((y_test != predictions) & (predictions == 1))\n",
    "    FN = np.sum((y_test != predictions) & (predictions == 0))\n",
    "    TP = np.sum((y_test == predictions) & (predictions == 1))\n",
    "    TN = np.sum((y_test == predictions) & (predictions == 0))\n",
    "\n",
    "\n",
    "    # Results\n",
    "    print('/n', model.__class__.__name__)\n",
    "    print('Acc :', (TP + TN) / (TP + TN + FP + FN))\n",
    "    print('FP :', FP)\n",
    "    print('TP :', TP)\n",
    "    print('FN :', FN)\n",
    "    print('TN :', TN)\n",
    "    \n",
    "    print('Pred_pos :',np.sum(predictions == 1))\n",
    "    print('Pred_neg :',np.sum(predictions == 0))\n",
    "    print('Total_pos :',np.sum(y_test == 1))\n",
    "    print('Total_neg :',np.sum(y_test == 0))\n",
    "\n",
    "    print('Precision : {} \\nRecall : {} \\nF1 {}'.format(precision,recall,f1))\n",
    "        \n",
    "def run_pipeline(model_list,scaler=None,downsample_size=.01):\n",
    "    \"\"\"Function to call each individual pipeline step\"\"\"\n",
    "    train,test,y_train,y_test,indexer = preprocess_dataset(load_data(),downsample_size)\n",
    "    trained_models = train_models(model_list,train,y_train,None,save=False)\n",
    "    prediction_list = []\n",
    "    for model in trained_models:\n",
    "        predictions = binarize_predictions(\n",
    "                make_probability_predictions(model,test),.5)\n",
    "        prediction_list.append(predictions)\n",
    "        model_accuracies(y_test,predictions,model)\n",
    "    return prediction_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "../Data/Processed/CA_S2Cells/CA_S2Cells.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ../Data/Processed/CA_S2Cells/CA_S2Cells.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-ec7315d345a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mca_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data/Processed/CA_S2Cells/CA_S2Cells.shp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mca_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'epsg:4326'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mca_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'S2_Cells_I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'S2_Cells_ID'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mca_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, bbox, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 253\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ../Data/Processed/CA_S2Cells/CA_S2Cells.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "ca_df = gpd.read_file(\"../Data/Processed/CA_S2Cells/CA_S2Cells.shp\")\n",
    "ca_df = ca_df.to_crs({'init': 'epsg:4326'})\n",
    "ca_df.rename(columns={'S2_Cells_I': 'S2_Cells_ID'}, inplace=True)\n",
    "ca_df.shape\n",
    "\n",
    "selected_date = '2018-11-08'\n",
    "ca_preds_df = ca_df.merge(ca_wf[ca_wf.WF_ALARM_DATE_DT_DT==selected_date][['S2_Cells_ID','WF_WildFire']], on='S2_Cells_ID', how='left')\\\n",
    "                   .merge(mlp_adam_noprecip_cw2000_probs[mlp_adam_noprecip_cw2000_probs.Date==selected_date].drop(columns='Date'), on='S2_Cells_ID')\\\n",
    "                   .merge(mlp_adam_precip_cw2000_probs[mlp_adam_precip_cw2000_probs.Date==selected_date].drop(columns='Date'), on='S2_Cells_ID')\\\n",
    "                   .merge(mlp_adam_noprecip_cwEQ_ext_probs[mlp_adam_noprecip_cwEQ_ext_probs.Date==selected_date].drop(columns='Date'), on='S2_Cells_ID')\\\n",
    "                   .merge(mlp_adam_precip_cwEQ_ext_probs[mlp_adam_precip_cwEQ_ext_probs.Date==selected_date].drop(columns='Date'), on='S2_Cells_ID')\\\n",
    "                   .fillna(0)\n",
    "ca_preds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../full_dataset/consolidated-data-000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "# Models to try\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Initialize the model with defaults\n",
    "lr = LogisticRegression()\n",
    "ifc = IsolationForest(contamination=.001)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# add any additional models to model_list\n",
    "model_list = [lr,ifc,rf]\n",
    "predictions = run_pipeline(model_list)\n",
    "\n",
    "# Check column names and types\n",
    "# for col in wildfire_data.columns:\n",
    "#     print('Name :', col,'dtyp : ', wildfire_data[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(488051, 16)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# train,test,y_train,y_test,indexer = preprocess_dataset(load_data(),.01)\n",
    "def train_model(num_layers,num_epochs=1,dropout_rate=.2):\n",
    "    input_dim = train.shape[1]\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(input_dim, activation='relu'))\n",
    "    model.add(layers.Reshape((2,8)))\n",
    "    model.add(layers.Conv1D(32,2, activation='relu'))\n",
    "\n",
    "    for layer in range(num_layers -1):\n",
    "        model.add(layers.Conv1D(32,1, activation='relu'))\n",
    "        model.add(layers.Dropout(rate=dropout_rate))\n",
    "    # model.add(layers.Conv1D(32,1, activation='relu'))\n",
    "    # model.add(layers.Conv1D(32,1, activation='relu'))\n",
    "    # model.add(layers.Conv1D(32,1, activation='relu'))\n",
    "    # model.add(layers.Conv1D(32,1, activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(.01),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=[tf.keras.metrics.Precision(),\n",
    "                          tf.keras.metrics.Recall()])\n",
    "    # train1 = np.vstack(train.values)\n",
    "    model.fit(train.values,y_train, epochs=num_epochs,\n",
    "              validation_data=(test.values,y_test))\n",
    "\n",
    "    print(model.evaluate(test.values,y_test))\n",
    "    return model.predict_proba(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5023 samples, validate on 243449 samples\n",
      "Epoch 1/10\n",
      "5023/5023 [==============================] - 23s 5ms/sample - loss: 15.4853 - precision_33: 0.0287 - recall_33: 1.0000 - val_loss: 15.9374 - val_precision_33: 3.1218e-04 - val_recall_33: 1.0000\n",
      "Epoch 2/10\n",
      "5023/5023 [==============================] - 23s 5ms/sample - loss: 15.4853 - precision_33: 0.0287 - recall_33: 1.0000 - val_loss: 15.9374 - val_precision_33: 3.1218e-04 - val_recall_33: 1.0000\n",
      "Epoch 3/10\n",
      "5023/5023 [==============================] - 22s 4ms/sample - loss: 15.4853 - precision_33: 0.0287 - recall_33: 1.0000 - val_loss: 15.9374 - val_precision_33: 3.1218e-04 - val_recall_33: 1.0000\n",
      "Epoch 4/10\n",
      "5023/5023 [==============================] - 23s 5ms/sample - loss: 15.4853 - precision_33: 0.0287 - recall_33: 1.0000 - val_loss: 15.9374 - val_precision_33: 3.1218e-04 - val_recall_33: 1.0000\n",
      "Epoch 5/10\n",
      "5023/5023 [==============================] - 22s 4ms/sample - loss: 15.4853 - precision_33: 0.0287 - recall_33: 1.0000 - val_loss: 15.9374 - val_precision_33: 3.1218e-04 - val_recall_33: 1.0000\n",
      "Epoch 6/10\n",
      "5023/5023 [==============================] - 22s 4ms/sample - loss: 15.4853 - precision_33: 0.0287 - recall_33: 1.0000 - val_loss: 15.9374 - val_precision_33: 3.1218e-04 - val_recall_33: 1.0000\n",
      "Epoch 7/10\n",
      "4832/5023 [===========================>..] - ETA: 0s - loss: 15.4772 - precision_33: 0.0292 - recall_33: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-b65c251dedbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.flatten()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-41ac216d2922>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(num_layers, num_epochs, dropout_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# train1 = np.vstack(train.values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     model.fit(train.values,y_train, epochs=num_epochs,\n\u001b[0;32m---> 28\u001b[0;31m               validation_data=(test.values,y_test))\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m           validation_in_fit=True)\n\u001b[0m\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions1 = train_model(5,10)#.flatten()\n",
    "\n",
    "plt.hist(predictions1)\n",
    "\n",
    "predictions1.flatten()\n",
    "# model_accuracies(y_test,predictions1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_accuracies(y_test,model.predict(test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
