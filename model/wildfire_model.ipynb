{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudstorage as gcs\n",
    "import glob\n",
    "import gc\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas_gbq\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from joblib import dump,load\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load each csv into a pandas DF\n",
    "\n",
    "def load_data(path=None):\n",
    "    \"\"\"Loads the csvs into pandas df\n",
    "    \n",
    "    Kwargs:\n",
    "        path - str - path to csv data files\n",
    "    Returns:\n",
    "        wildfire_data - Pandas DF - DF of all data\"\"\"\n",
    "    \n",
    "    if not path:\n",
    "        path = r'../../full_dataset' # use your path\n",
    "        \n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    li = []\n",
    "    i = 0\n",
    "    for filename in all_files:\n",
    "        print(filename)\n",
    "        if i == 0:\n",
    "            wildfire_data = pd.read_csv(filename)\n",
    "        else:\n",
    "            wildfire_data = wildfire_data.append(pd.read_csv(filename))\n",
    "        i += 1\n",
    "        print(wildfire_data.shape)\n",
    "    \n",
    "    wildfire_data.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return wildfire_data\n",
    "\n",
    "def preprocess_dataset(wildfire_data,downsample_size=None):\n",
    "    \"\"\"Fills in NA's, creates train/test split, removes unused cols\n",
    "    Args :\n",
    "        wildfire_data - Pandas DF - consolidated dataset\n",
    "    Returns :\n",
    "        train - Pandas DF - 2016-2017 input data\n",
    "        test - Pandas DF - 2018 input data\n",
    "        y_train - numpy array - 2016-2017 labels\n",
    "        y_test - numpy array - 2018 labels \n",
    "        indexer - Pandas DF - lookup for results Data/S2Cell by index\n",
    "        downsample_size - float - percentage of negative samples to include in Train\n",
    "    \"\"\"\n",
    "    \n",
    "    fuel_mean = 78.74 # Calculated in BigQuery\n",
    "    wildfire_data.fuel_percent.replace('backfill',str(fuel_mean),inplace = True)\n",
    "    wildfire_data['fuel_percent'] = pd.to_numeric(wildfire_data.fuel_percent)\n",
    "\n",
    "    # DF used to resolve which date/s2 cell each prediction corresponds too\n",
    "    indexer = wildfire_data[['s2_cell_id','measure_date']].copy()\n",
    "\n",
    "    train = wildfire_data[wildfire_data.measure_date < '2018-01-01'].copy()\n",
    "    test = wildfire_data[wildfire_data.measure_date >= '2018-01-01'].copy()\n",
    "\n",
    "    if downsample_size:\n",
    "        train = train[train.wf_wildfire.fillna(0) == 0].sample(frac=downsample_size).append(train[train.wf_wildfire > 0])\n",
    "        gc.collect()\n",
    "    y_train = train.wf_wildfire.fillna(0).copy().values\n",
    "    y_test = test.wf_wildfire.fillna(0).copy().values\n",
    "    \n",
    "    y_train[y_train > 0] = 1\n",
    "    y_test[y_test > 0] = 1\n",
    "    \n",
    "    # TODO identify any additional/engineered features to include\n",
    "    feature_cols = ['tl_object_id','fuel_percent',\n",
    "                    'wea_air_temp_max', 'wea_air_temp_mean', \n",
    "                    'wea_precip_accum_max', 'relative_humidity_max',\n",
    "                    'relative_humidity_min', 'relative_humidity_mean',\n",
    "                    'wea_wind_speed_max', 'wea_wind_speed_min', 'wea_wind_speed_mean',\n",
    "                    'wind_gust_max',\n",
    "                    'sat_faparval_min',\n",
    "                    'sat_faparval_max', 'sat_faparval_mean', \n",
    "                    'sat_faparval_median'\n",
    "                   ]\n",
    "\n",
    "    train = train[feature_cols]\n",
    "    test = test[feature_cols]\n",
    "\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "    train['fuel_percent'] = pd.to_numeric(train.fuel_percent)\n",
    "    test['fuel_percent'] = pd.to_numeric(test.fuel_percent)\n",
    "\n",
    "    del wildfire_data\n",
    "    gc.collect()\n",
    "\n",
    "    return train,test,y_train,y_test,indexer\n",
    "\n",
    "def train_models(models,train_data,train_labels,scaler=None,save=True):\n",
    "    \"\"\"Given a list of Sklearn models returns a list of trained models\n",
    "    \n",
    "    Args:\n",
    "        model - list of Sklearn model objects - models to be trained\n",
    "        train_data - Pandas DF - preprocessed training data\n",
    "        train_labels - Numpy Array - training data labels\n",
    "        scaler - Sklearn Scaler Object - [Optional]  \n",
    "        save - Bool - if true writes the models to disk \n",
    "    \"\"\"\n",
    "    trained_models = []\n",
    "    model_path = 'wildfire_{}.joblib'\n",
    "    if save == False:\n",
    "        trained_models = [load(model) for model in glob.glob('*.joblib')]\n",
    "    else:\n",
    "        if scaler:\n",
    "            for model in models:\n",
    "                model_name = model.__class__.__name__\n",
    "                print('Training -',model_name)\n",
    "                t0 = time.time()\n",
    "\n",
    "                trained_models.append(\n",
    "                    model.fit(scaler.fit_transform(train_data),train_labels))\n",
    "\n",
    "                dump(trained_models[-1], (model_path.format(model_name)))\n",
    "                print('Training time -',str(round(time.time() - t0,2))+'s')\n",
    "\n",
    "        else:\n",
    "            for model in models:\n",
    "                trained_models.append(\n",
    "                    model.fit(train_data,train_labels))\n",
    "\n",
    "                dump(trained_models[-1], (model_path.format(model_name)))\n",
    "    return trained_models\n",
    "    \n",
    "\n",
    "def make_probability_predictions(model,test_data):\n",
    "    \"\"\"Makes probability predictions on the validation data for each model\"\"\"\n",
    "    \n",
    "    if model.__class__.__name__ == 'IsolationForest':\n",
    "        predictions = ifc.decision_function(scaler.transform(test_data))\n",
    "    else:\n",
    "        predictions = model.predict_proba(test_data)\n",
    "    return predictions\n",
    "\n",
    "def binarize_predictions(predictions,threshold=.5):\n",
    "    \"\"\"Implements a custom classification threshold\"\"\"\n",
    "    \n",
    "    pred = np.zeros(len(predictions))\n",
    "    try:\n",
    "        if predictions.shape[1] == 2:\n",
    "            pred[[predictions[:,1] > threshold]] = 1\n",
    "    except: \n",
    "        pred[predictions > threshold] = 1\n",
    "    return pred\n",
    "\n",
    "def model_accuracies(y_test,predictions, model=None):\n",
    "    \"\"\"Calculates a variety of accuracy metrics\n",
    "    \n",
    "    Args:\n",
    "        y_test - Numpy Array - true labels\n",
    "        predictions - Numpy Array - binarized (0,1) model outputs\n",
    "        model - Sklearn Classifier \n",
    "    \n",
    "    \"\"\"\n",
    "    # # Accuracy Metrics\n",
    "    f1 = f1_score(y_test,predictions)\n",
    "    precision = precision_score(y_test,predictions)\n",
    "    recall = recall_score(y_test,predictions)\n",
    "\n",
    "    FP = np.sum((y_test != predictions) & (predictions == 1))\n",
    "    FN = np.sum((y_test != predictions) & (predictions == 0))\n",
    "    TP = np.sum((y_test == predictions) & (predictions == 1))\n",
    "    TN = np.sum((y_test == predictions) & (predictions == 0))\n",
    "\n",
    "\n",
    "    # Results\n",
    "    print('/n', model.__class__.__name__)\n",
    "    print('Acc :', (TP + TN) / (TP + TN + FP + FN))\n",
    "    print('FP :', FP)\n",
    "    print('TP :', TP)\n",
    "    print('FN :', FN)\n",
    "    print('TN :', TN)\n",
    "    \n",
    "    print('Pred_pos :',np.sum(predictions == 1))\n",
    "    print('Pred_neg :',np.sum(predictions == 0))\n",
    "    print('Total_pos :',np.sum(y_test == 1))\n",
    "    print('Total_neg :',np.sum(y_test == 0))\n",
    "\n",
    "    print('Precision : {} \\nRecall : {} \\nF1 {}'.format(precision,recall,f1))\n",
    "        \n",
    "def run_pipeline(model_list,scaler=None,downsample_size=.01):\n",
    "    \"\"\"Function to call each individual pipeline step\"\"\"\n",
    "    train,test,y_train,y_test,indexer = preprocess_dataset(load_data(),downsample_size)\n",
    "    trained_models = train_models(model_list,train,y_train,scaler,save=True)\n",
    "    for model in trained_models:\n",
    "        predictions = binarize_predictions(\n",
    "                make_probability_predictions(model,test),.5)\n",
    "        model_accuracies(y_test,predictions,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../full_dataset/consolidated-data-000000000000.csv\n",
      "(731500, 151)\n",
      "../../full_dataset/consolidated-data-000000000012.csv\n",
      "(1460002, 151)\n",
      "../../full_dataset/consolidated-data-000000000010.csv\n",
      "(2187622, 151)\n",
      "../../full_dataset/consolidated-data-000000000004.csv\n",
      "(2915744, 151)\n",
      "../../full_dataset/consolidated-data-000000000008.csv\n",
      "(3644593, 151)\n",
      "../../full_dataset/consolidated-data-000000000001.csv\n",
      "(4374684, 151)\n",
      "../../full_dataset/consolidated-data-000000000005.csv\n",
      "(5104524, 151)\n",
      "../../full_dataset/consolidated-data-000000000011.csv\n",
      "(5834827, 151)\n",
      "../../full_dataset/consolidated-data-000000000003.csv\n",
      "(6564054, 151)\n",
      "../../full_dataset/consolidated-data-000000000002.csv\n",
      "(7292172, 151)\n",
      "../../full_dataset/consolidated-data-000000000014.csv\n",
      "(8021810, 151)\n",
      "../../full_dataset/consolidated-data-000000000007.csv\n",
      "(8750401, 151)\n",
      "../../full_dataset/consolidated-data-000000000006.csv\n",
      "(9480922, 151)\n",
      "../../full_dataset/consolidated-data-000000000009.csv\n",
      "(10208815, 151)\n",
      "../../full_dataset/consolidated-data-000000000013.csv\n",
      "(10935879, 151)\n",
      "../../full_dataset/consolidated-data-000000000015.csv\n",
      "(11664728, 151)\n",
      "Training - LogisticRegression\n",
      "Training time - 0.66s\n",
      "Training - IsolationForest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nconidas_berkeley_edu/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time - 3.71s\n",
      "Training - RandomForestClassifier\n",
      "Training time - 2.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nconidas_berkeley_edu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:140: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n LogisticRegression\n",
      "Acc : 0.8381618119311812\n",
      "FP : 627760\n",
      "TP : 310\n",
      "FN : 932\n",
      "TN : 3255693\n",
      "Pred_pos : 628070\n",
      "Pred_neg : 3256625\n",
      "Total_pos : 1242\n",
      "Total_neg : 3883453\n",
      "Precision : 0.0004935755568646807 \n",
      "Recall : 0.249597423510467 \n",
      "F1 0.0009852028882334992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nconidas_berkeley_edu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/nconidas_berkeley_edu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n IsolationForest\n",
      "Acc : 0.9996802837803225\n",
      "FP : 0\n",
      "TP : 0\n",
      "FN : 1242\n",
      "TN : 3883453\n",
      "Pred_pos : 0\n",
      "Pred_neg : 3884695\n",
      "Total_pos : 1242\n",
      "Total_neg : 3883453\n",
      "Precision : 0.0 \n",
      "Recall : 0.0 \n",
      "F1 0.0\n",
      "/n RandomForestClassifier\n",
      "Acc : 0.8536075547758576\n",
      "FP : 567722\n",
      "TP : 274\n",
      "FN : 968\n",
      "TN : 3315731\n",
      "Pred_pos : 567996\n",
      "Pred_neg : 3316699\n",
      "Total_pos : 1242\n",
      "Total_neg : 3883453\n",
      "Precision : 0.00048239776336453074 \n",
      "Recall : 0.22061191626409019 \n",
      "F1 0.0009626904739318176\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "# Models to try\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Initialize the model with defaults\n",
    "lr = LogisticRegression()\n",
    "ifc = IsolationForest(contamination=.1)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# add any additional models to model_list\n",
    "model_list = [lr,ifc,rf]\n",
    "run_pipeline(model_list,scaler)\n",
    "\n",
    "# Check column names and types\n",
    "# for col in wildfire_data.columns:\n",
    "#     print('Name :', col,'dtyp : ', wildfire_data[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for outputting the predictions to GCP\n",
    "# test1 = test.copy()\n",
    "# test1['predictions'] = predictions\n",
    "# test1 = test1.merge(indexer,how='left',left_index=True,right_index=True)\n",
    "# test1.head()\n",
    "# Write the predictions to gcp\n",
    "# indexer.to_gbq('fuel_moisture_by_s2_cell.lr_predictions',\n",
    "#                 'neon-obelisk-215514',\n",
    "#                 if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv1d_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-46c2179df39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# train1 = np.vstack(train.values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m model.fit(train.values,y_train, epochs=3,\n\u001b[0;32m---> 19\u001b[0;31m           validation_data=(test.values,y_test))\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m       \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2527\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2529\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2531\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     outputs, _ = self._call_and_compute_mask(\n\u001b[0;32m--> 233\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m_call_and_compute_mask\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    252\u001b[0m           \u001b[0;31m# Build layer if applicable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m           \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     input_spec.assert_input_compatibility(\n\u001b[0;32m-> 1591\u001b[0;31m         self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   1592\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv1d_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 16]"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "# train,test,y_train,y_test,indexer = preprocess_dataset(load_data(),1)\n",
    "\n",
    "input_dim = train.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(input_dim, activation='relu'))\n",
    "model.add(layers.Conv1D(32,1, activation='relu',\n",
    "                       input_shape=[1,16,1]))\n",
    "model.add(layers.Dense(1, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=[tf.keras.metrics.Precision(),\n",
    "                      tf.keras.metrics.Recall()])\n",
    "# train1 = np.vstack(train.values)\n",
    "model.fit(train.values,y_train, epochs=3,\n",
    "          validation_data=(test.values,y_test))\n",
    "\n",
    "print(model.evaluate(test.values,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0.,      0.,      0.,      0.,      0., 243449.,      0.,\n",
       "             0.,      0.,      0.]),\n",
       " array([0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3, 1.4, 1.5],\n",
       "       dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions1 = model.predict_proba(test.values)\n",
    "\n",
    "plt.hist(predictions1)\n",
    "# model_accuracies(y_test,predictions1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
