{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudstorage as gcs\n",
    "import glob\n",
    "import gc\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas_gbq\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from joblib import dump,load\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load each csv into a pandas DF\n",
    "\n",
    "def load_data(path=None):\n",
    "    \"\"\"Loads the csvs into pandas df\n",
    "    \n",
    "    Kwargs:\n",
    "        path - str - path to csv data files\n",
    "    Returns:\n",
    "        wildfire_data - Pandas DF - DF of all data\"\"\"\n",
    "    \n",
    "    if not path:\n",
    "        path = r'../../full_dataset' # use your path\n",
    "        \n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    li = []\n",
    "    i = 0\n",
    "    for filename in all_files:\n",
    "        print(filename)\n",
    "        if i == 0:\n",
    "            wildfire_data = pd.read_csv(filename)\n",
    "        else:\n",
    "            wildfire_data = wildfire_data.append(pd.read_csv(filename))\n",
    "        i += 1\n",
    "        print(wildfire_data.shape)\n",
    "    \n",
    "    wildfire_data.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return wildfire_data\n",
    "\n",
    "def preprocess_dataset(wildfire_data):\n",
    "    \"\"\"Fills in NA's, creates train/test split, removes unused cols\n",
    "    Args :\n",
    "        wildfire_data - Pandas DF - consolidated dataset\n",
    "    Returns :\n",
    "        train - Pandas DF - 2016-2017 input data\n",
    "        test - Pandas DF - 2018 input data\n",
    "        y_train - numpy array - 2016-2017 labels\n",
    "        y_test - numpy array - 2018 labels \n",
    "        indexer - Pandas DF - lookup for results Data/S2Cell by index\n",
    "    \"\"\"\n",
    "    \n",
    "    fuel_mean = 78.74 # Calculated in BigQuery\n",
    "    wildfire_data.fuel_percent.replace('backfill',str(fuel_mean),inplace = True)\n",
    "    wildfire_data['fuel_percent'] = pd.to_numeric(wildfire_data.fuel_percent)\n",
    "\n",
    "    # DF used to resolve which date/s2 cell each prediction corresponds too\n",
    "    indexer = wildfire_data[['s2_cell_id','measure_date']].copy()\n",
    "\n",
    "    train = wildfire_data[wildfire_data.measure_date < '2018-01-01'].copy()\n",
    "    test = wildfire_data[wildfire_data.measure_date >= '2018-01-01'].copy()\n",
    "\n",
    "    y_train = train.wf_wildfire.fillna(0).copy().values\n",
    "    y_test = test.wf_wildfire.fillna(0).copy().values\n",
    "\n",
    "    # TODO identify any additional/engineered features to include\n",
    "    feature_cols = ['tl_object_id','fuel_percent',\n",
    "                    'wea_air_temp_max', 'wea_air_temp_mean', \n",
    "                    'wea_precip_accum_max', 'relative_humidity_max',\n",
    "                    'relative_humidity_min', 'relative_humidity_mean',\n",
    "                    'wea_wind_speed_max', 'wea_wind_speed_min', 'wea_wind_speed_mean',\n",
    "                    'wind_gust_max',\n",
    "#                     'sat_faparval_min',\n",
    "#                     'sat_faparval_max', 'sat_faparval_mean', \n",
    "#                     'sat_faparval_median'\n",
    "                   ]\n",
    "\n",
    "    train = train[feature_cols]\n",
    "    test = test[feature_cols]\n",
    "\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "    train['fuel_percent'] = pd.to_numeric(train.fuel_percent)\n",
    "    test['fuel_percent'] = pd.to_numeric(test.fuel_percent)\n",
    "\n",
    "    del wildfire_data\n",
    "    gc.collect()\n",
    "\n",
    "    return train,test,y_train,y_test,indexer\n",
    "\n",
    "def train_models(models,train_data,train_labels,scaler=None,save=False):\n",
    "    \"\"\"Given a list of Sklearn models returns a list of trained models\n",
    "    \n",
    "    Args:\n",
    "        model - list of Sklearn model objects - models to be trained\n",
    "        train_data - Pandas DF - preprocessed training data\n",
    "        train_labels - Numpy Array - training data labels\n",
    "        scaler - Sklearn Scaler Object - [Optional]  \n",
    "        save - Bool - if true writes the models to disk \n",
    "    \"\"\"\n",
    "    trained_models = []\n",
    "    if scaler:\n",
    "        for model in models:\n",
    "            print('Training -',model.__class__.__name__)\n",
    "            t0 = time.time()\n",
    "            trained_models.append(\n",
    "                model.fit(scaler.fit_transform(train_data),train_labels))\n",
    "            if save:\n",
    "                dump(trained_models[-1], ('../../wildfire_lr.joblib'))\n",
    "            print('Training time -',str(round(time.time() - t0,2))+'s')\n",
    "\n",
    "    else:\n",
    "        for model in models:\n",
    "            trained_models.append(\n",
    "                model.fit(train_data,train_labels))\n",
    "            \n",
    "            if save:\n",
    "                dump(trained_models[-1], ('../../wildfire_lr.joblib'))\n",
    "    return trained_models\n",
    "    \n",
    "\n",
    "def make_probability_predictions(model,test_data):\n",
    "    \"\"\"Makes probability predictions on the validation data for each model\"\"\"\n",
    "    \n",
    "    if model.__class__.__name__ == 'IsolationForest':\n",
    "        predictions = ifc.decision_function(scaler.transform(test_data))\n",
    "    else:\n",
    "        predictions = model.predict_proba(test_data)\n",
    "    return predictions\n",
    "\n",
    "def binarize_predictions(predictions,threshold=.5):\n",
    "    \"\"\"Implements a custom classification threshold\"\"\"\n",
    "    \n",
    "    pred = np.zeros(len(predictions))\n",
    "    try:\n",
    "        if predictions.shape[1] == 2:\n",
    "            pred[[predictions[:,1] > threshold]] = 1\n",
    "    except: \n",
    "        pred[predictions > threshold] = 1\n",
    "    return pred\n",
    "\n",
    "\n",
    "def model_accuracies(y_test,predictions, model):\n",
    "    \"\"\"Calculates a variety of accuracy metrics\n",
    "    \n",
    "    Args:\n",
    "        y_test - Numpy Array - true labels\n",
    "        predictions - Numpy Array - binarized (0,1) model outputs\n",
    "        model - Sklearn Classifier \n",
    "    \n",
    "    \"\"\"\n",
    "    # # Accuracy Metrics\n",
    "    f1 = f1_score(y_test,predictions)\n",
    "    precision = precision_score(y_test,predictions)\n",
    "    recall = recall_score(y_test,predictions)\n",
    "\n",
    "    FP = np.sum((y_test != predictions) & (predictions == 1))\n",
    "    FN = np.sum((y_test != predictions) & (predictions == 0))\n",
    "    TP = np.sum((y_test == predictions) & (predictions == 1))\n",
    "    TN = np.sum((y_test == predictions) & (predictions == 0))\n",
    "\n",
    "\n",
    "    # Results\n",
    "    print('/n', model.__class__.__name__)\n",
    "    print('Acc :', (TP + TN) / (TP + TN + FP + FN))\n",
    "    print('FP :', FP)\n",
    "    print('TP :', TP)\n",
    "    print('FN :', FN)\n",
    "    print('TN :', TN)\n",
    "    \n",
    "    print('Pred_pos :',np.sum(predictions == 1))\n",
    "    print('Pred_neg :',np.sum(predictions == 0))\n",
    "    print('Total_pos :',np.sum(y_test == 1))\n",
    "    print('Total_neg :',np.sum(y_test == 0))\n",
    "\n",
    "    print('Precision : {} \\nRecall : {} \\nF1 {}'.format(precision,recall,f1))\n",
    "\n",
    "    \n",
    "def run_pipeline(model_list,scaler=None):\n",
    "    \"\"\"Function to call each individual pipeline step\"\"\"\n",
    "    train,test,y_train,y_test,indexer = preprocess_dataset(load_data())\n",
    "    trained_models = train_models(model_list,train,y_train,scaler,save=True)\n",
    "    for model in trained_models:\n",
    "        predictions = binarize_predictions(\n",
    "                make_probability_predictions(model,test),.008)\n",
    "        model_accuracies(y_test,predictions,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../full_dataset/consolidated-data-000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "# Models to try\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Initialize the model with defaults\n",
    "lr = LogisticRegression()\n",
    "ifc = IsolationForest()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# add any additional models to model_list\n",
    "model_list = [lr,ifc,rf]\n",
    "run_pipeline(model_list,scaler)\n",
    "\n",
    "# Check column names and types\n",
    "# for col in wildfire_data.columns:\n",
    "#     print('Name :', col,'dtyp : ', wildfire_data[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for outputting the predictions to GCP\n",
    "# test1 = test.copy()\n",
    "# test1['predictions'] = predictions\n",
    "# test1 = test1.merge(indexer,how='left',left_index=True,right_index=True)\n",
    "# test1.head()\n",
    "# Write the predictions to gcp\n",
    "# indexer.to_gbq('fuel_moisture_by_s2_cell.lr_predictions',\n",
    "#                 'neon-obelisk-215514',\n",
    "#                 if_exists = 'replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
